{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP-Hp14Kp429"
      },
      "source": [
        "# Running LLMs Locally with Ollama in Google Colab\n",
        "\n",
        "Welcome! This notebook will guide you through the process of setting up and running powerful Large Language Models (LLMs) right here in this Colab environment using a fantastic tool called [Ollama](https://ollama.com/).\n",
        "\n",
        "**What is Ollama?**\n",
        "\n",
        "Ollama is a tool that makes it incredibly easy to download, run, and manage open-source LLMs like Llama 3, Phi-3, and more on your own machine (or in this case, a Colab notebook). It handles all the complex setup, allowing you to focus on interacting with the models.\n",
        "\n",
        "**What to Expect (40-Minute Agenda):**\n",
        "*   **Part 1 (10 mins):** Setup and Installation\n",
        "*   **Part 2 (15 mins):** Basic LLM Interaction (Generation & Chat)\n",
        "*   **Part 3 (10 mins):** Multimodality & Model Customization\n",
        "*   **Part 4 (5 mins):** Wrap-up and Resources\n",
        "\n",
        "**Assistance**\n",
        "\n",
        "If you get stuck, please ask an assistant or use the gemini interface at the bottom of the screen!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWvLc8MJp8aL"
      },
      "source": [
        "## Part 1: Setup and Installation\n",
        "\n",
        "Let's start by installing Ollama on our Colab instance. We'll use a `curl` command to download and run the installation script. This is the standard way to install Ollama on Mac or Linux."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UomtUgNqC31"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuSmCld3qL5X"
      },
      "source": [
        "Ollama runs as a background server. In a standard terminal, you would just run `ollama serve`. However, in Colab, running it directly would block the notebook from executing other cells.\n",
        "\n",
        "To get around this, we'll use Python's `threading` and `subprocess` modules to start the Ollama server in a separate thread. This lets it run in the background so we can continue with our notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhEvbJaZqOY9"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  \"\"\"Starts the Ollama server as a background process.\"\"\"\n",
        "  command = \"ollama serve\"\n",
        "  # Using subprocess.Popen to run the command in the background\n",
        "  process = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  # You can optionally print the output for debugging\n",
        "  # for line in iter(process.stdout.readline, b''):\n",
        "  #   print(line.decode('utf-8'), end='')\n",
        "\n",
        "# Start the Ollama server in a new thread\n",
        "ollama_thread = threading.Thread(target=run_ollama_serve)\n",
        "ollama_thread.start()\n",
        "\n",
        "# Give the server a moment to start up\n",
        "time.sleep(5)\n",
        "print(\"Ollama server started.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yj77zZkqXwL"
      },
      "source": [
        "## Part 2: Basic LLM Interaction\n",
        "\n",
        "With the server running, we can now download a model from the [Ollama Model Library](https://ollama.com/library).\n",
        "\n",
        "For our first model, let's use `phi3:mini`. It's a powerful yet small model, which makes it fast to download and run. We use the `ollama pull` command for this.\n",
        "\n",
        "Some actions take a while, so [read this while you are waiting](https://www.ibm.com/think/topics/multimodal-llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNZqyR44qT62"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.2:1b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlzmYYJ0qdHS"
      },
      "source": [
        "To interact with our running Ollama server using Python, we'll install the official `ollama` Python library. This library provides a simple and convenient way to send prompts and receive responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wuVoa4KqhRs"
      },
      "outputs": [],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BISSFMtq-ir"
      },
      "source": [
        "Now for the fun part! Let's generate some text. We'll import the `ollama` library and use the `generate` function. We just need to specify the model we want to use and provide a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYR6YOUSrDFC"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "response = ollama.generate(\n",
        "    model='llama3.2:1b',\n",
        "    prompt='Why is the sky blue?'\n",
        ")\n",
        "\n",
        "print(response['response'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEzX6FthrGgM"
      },
      "source": [
        "Ollama also supports conversational chat. The `chat` function allows you to send a series of messages with different roles (`user` and `assistant`) to maintain context. Let's have a short conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nY41H8ErHwm"
      },
      "outputs": [],
      "source": [
        "chat_history = [\n",
        "    {'role': 'user', 'content': 'What is the capital of France?'}\n",
        "]\n",
        "\n",
        "response = ollama.chat(model='llama3.2:1b', messages=chat_history)\n",
        "\n",
        "print(response['message']['content'])\n",
        "\n",
        "# Add the assistant's response to the history\n",
        "chat_history.append(response['message'])\n",
        "\n",
        "# Ask a follow-up question\n",
        "chat_history.append({'role': 'user', 'content': 'What is a famous landmark there?'})\n",
        "\n",
        "follow_up_response = ollama.chat(model='llama3.2:1b', messages=chat_history)\n",
        "\n",
        "print(\"\\nFollow-up response:\")\n",
        "print(follow_up_response['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxSah2KcrQzs"
      },
      "source": [
        "Some LLMs are **multimodal**, meaning they can understand more than just text, like images! `llava` is a popular model for this. Let's pull the llava model and give it an image to describe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DplXUx7yrLQ2"
      },
      "outputs": [],
      "source": [
        "# Pull the llava model (this may take a few minutes)\n",
        "!ollama pull llava"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9kMNOeNzZ7L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# --- Step 1: Download a sample image ---\n",
        "# NOTE: Using a direct URL to an image file is crucial.\n",
        "# The previous Google link was a redirect and would not download the image correctly.\n",
        "print(\"Downloading image...\")\n",
        "!wget -O \"sample_image.jpg\" \"https://limaspanishhouse.com/wp-content/uploads/2021/02/peruvian-llama-2-2048x1795.jpg\"\n",
        "\n",
        "# --- Step 2: Verify the download ---\n",
        "# Let's check the file size. A non-zero size means it downloaded successfully.\n",
        "print(\"\\nVerifying downloaded file:\")\n",
        "!ls -l sample_image.jpg\n",
        "\n",
        "# --- Step 3: Display the image with Matplotlib if it exists ---\n",
        "file_path = 'sample_image.jpg'\n",
        "if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
        "    print(\"\\nDownload successful. Displaying image using Matplotlib:\")\n",
        "    try:\n",
        "        # Read the image file\n",
        "        img = mpimg.imread(file_path)\n",
        "\n",
        "        # Display the image\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')  # Hide the axes (x and y ticks) for a cleaner look\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- ERROR: Could not display the image. It might be corrupted. Error: {e} ---\")\n",
        "else:\n",
        "    print(\"\\n--- ERROR: Image download failed. The file is missing or empty. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx9OT0DbrZN2"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "import base64\n",
        "\n",
        "def image_to_base64(image_path):\n",
        "  \"\"\"Converts an image file to a base64 string.\"\"\"\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "image_b64 = image_to_base64(\"sample_image.jpg\")\n",
        "\n",
        "response = ollama.chat(\n",
        "    model='llava',\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Describe this image in detail.',\n",
        "            'images': [image_b64]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZPm7TOFreoW"
      },
      "source": [
        "You can create custom versions of models using a `Modelfile`. This allows you to set a specific persona or system prompt. Let's create a \"pirate\" version of `llama3.2B`.\n",
        "\n",
        "Note: This is the same as changing the system prompt on a website like chatgpt.com, just locally. This is what is happening under the hood.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vN93_B-rduU"
      },
      "outputs": [],
      "source": [
        "# # Define the Modelfile content\n",
        "# modelfile_content = \"\"\"\n",
        "# FROM llama3.2:3b\n",
        "# SYSTEM \"You are a friendly pirate. All your responses must be in the style of a pirate.\"\n",
        "# \"\"\"\n",
        "\n",
        "# # Write the Modelfile\n",
        "# with open(\"Modelfile\", \"w\") as f:\n",
        "#     f.write(modelfile_content)\n",
        "\n",
        "# # Create the custom model\n",
        "# !ollama create pirate-llama -f Modelfile\n",
        "\n",
        "# print(\"\\nCustom model 'pirate-llama' created!\")\n",
        "\n",
        "# # Now, let's chat with our pirate\n",
        "# response = ollama.chat(\n",
        "#     model='pirate-llama',\n",
        "#     messages=[{'role': 'user', 'content': 'What is the best thing about being a pirate?'}]\n",
        "# )\n",
        "\n",
        "# print(\"\\nPirate Response:\")\n",
        "# print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6tzPF33rq-J"
      },
      "source": [
        "## Part 4: Conclusion & Further Exploration\n",
        "\n",
        "Congratulations! You've successfully set up Ollama in Colab, generated text, chatted with a model, analyzed an image, and even created your own custom model.\n",
        "\n",
        "**Recap:**\n",
        "*   We installed and ran the Ollama server.\n",
        "*   We pulled models like `llama3.2B` and `llava`.\n",
        "*   We used the `ollama` Python library to `generate` and `chat`.\n",
        "*   We explored multimodal capabilities with `llava`.\n",
        "*   We created a custom persona with a `Modelfile`.\n",
        "\n",
        "**Where to go from here?**\n",
        "*   **Explore more models:** Check out the full [Ollama Library](https://ollama.com/library).\n",
        "*   **A research level explanation of transformers** [The Visual Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "*   **The original transformer paper** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "*   **Build an LLM from scratch** [Transformer from Scratch](https://arena-ch1-transformers.streamlit.app/[1.1]_Transformer_from_Scratch)\n",
        "\n",
        "*   **Build an App:** Try integrating Ollama into a simple web application using Flask or Streamlit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZ-y1jchTdA"
      },
      "source": [
        "## Extension: Understanding Text with Embeddings\n",
        "\n",
        "So far, we've used LLMs to generate text. But they can also be used to *understand* text. One of the most powerful ways to do this is by creating **embeddings**.\n",
        "\n",
        "**What is an Embedding?**\n",
        "\n",
        "An embedding is a list of numbers (a vector) that represents the meaning and context of a piece of text. Think of it like a coordinate on a giant \"meaning map.\" Texts with similar meanings will have coordinates that are very close to each other.\n",
        "\n",
        "We can use this to build a simple **semantic search** engine. Instead of matching keywords, it matches *meaning*.\n",
        "\n",
        "**Our Goal:**\n",
        "1. Create a small \"database\" of sentences.\n",
        "2. Convert these sentences into embeddings (numbers).\n",
        "3. Ask a question and convert it into an embedding.\n",
        "4. Find which sentence in our database is \"closest\" in meaning to our question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBOKS7rVhQUU"
      },
      "outputs": [],
      "source": [
        "!pip install -q scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsfjP0c6hcE4"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# --- Step 1: Create our \"database\" of sentences ---\n",
        "documents = [\n",
        "    \"The sun is the center of our solar system.\",\n",
        "    \"Lions are the kings of the jungle.\",\n",
        "    \"Photosynthesis is the process by which plants use sunlight to create food.\",\n",
        "    \"The stock market saw a significant downturn last week.\",\n",
        "    \"To make a good pasta, you must use high-quality ingredients.\"\n",
        "]\n",
        "\n",
        "# --- Step 2: Convert each sentence into an embedding ---\n",
        "# We'll use a special model optimized for creating embeddings. `mxbai-embed-large` is a great choice.\n",
        "print(\"Pulling the embedding model (mxbai-embed-large)...\")\n",
        "!ollama pull mxbai-embed-large\n",
        "\n",
        "print(\"\\nGenerating embeddings for our documents...\")\n",
        "doc_embeddings = []\n",
        "for doc in documents:\n",
        "    response = ollama.embeddings(model='mxbai-embed-large', prompt=doc)\n",
        "    doc_embeddings.append(response['embedding'])\n",
        "print(\"Embeddings created successfully.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Ask a question and create an embedding for it ---\n",
        "query = \"What do plants eat?\"\n",
        "print(f\"\\nQuery: '{query}'\")\n",
        "\n",
        "query_embedding_response = ollama.embeddings(model='mxbai-embed-large', prompt=query)\n",
        "query_embedding = query_embedding_response['embedding']\n",
        "\n",
        "\n",
        "# --- Step 4: Find the most similar document ---\n",
        "# We'll use cosine similarity. A smaller cosine distance means the texts are more similar.\n",
        "similarities = []\n",
        "for doc_emb in doc_embeddings:\n",
        "    # Calculate cosine distance (1 - similarity). We want the smallest distance.\n",
        "    distance = cosine(query_embedding, doc_emb)\n",
        "    similarities.append(distance)\n",
        "\n",
        "# Find the index of the document with the smallest distance\n",
        "most_similar_index = np.argmin(similarities)\n",
        "\n",
        "print(f\"\\nMost relevant document found: '{documents[most_similar_index]}'\")\n",
        "print(f\"(Cosine Distance: {similarities[most_similar_index]:.4f})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}